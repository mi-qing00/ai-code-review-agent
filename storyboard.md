# AI Code Review Agent - Development Plan

## Project Overview

**Goal:** Demonstrate backend SDE skills through event-driven architecture, async processing, and production-grade system design.

**Core Showcases:**
1. Event-driven webhook processing with job queue
2. Intelligent content-addressable caching
3. Comprehensive testing (unit + integration + load)

---

## Tech Stack

| Component | Technology |
|-----------|------------|
| Backend | Python 3.11 + FastAPI |
| Database | PostgreSQL 15 |
| Cache/Queue | Redis 7 (Streams + TTL cache) |
| LLM | OpenAI GPT-4 or Anthropic Claude |
| Testing | pytest + pytest-asyncio + locust |
| Deployment | Docker + Railway/Render |

---

## System Architecture

```
GitHub Webhook â†’ FastAPI â†’ Redis Queue â†’ Worker Pool â†’ GitHub
                    â†“           â†“            â†“            â†‘
                PostgreSQL   Cache      OpenAI API    Results
```

---

## Month 1: Core System (Weeks 1-4)

### Week 1: Foundation
- [x] Project setup (Poetry, FastAPI skeleton)
  - âœ… Poetry configuration with all dependencies
  - âœ… FastAPI application structure
  - âœ… Project structure (app/, migrations/, tests/)
  - âœ… Docker Compose setup
  - âœ… Git repository initialized and pushed to GitHub
  - âœ… Start script created
- [x] PostgreSQL schema (`pull_requests`, `reviews` tables)
  - âœ… Database schema created in `migrations/001_initial_schema.sql`
  - âœ… All tables and indexes created
  - âœ… Schema applied to database
- [x] Redis connection
  - âœ… Redis client module created (`app/db/redis_client.py`)
  - âœ… Connection pool management implemented
- [x] Environment config
  - âœ… Pydantic settings configuration
  - âœ… Environment variables setup
  - âœ… `.env` file template created
  - âœ… Configuration documentation

**Progress:** 100% complete âœ…
**Deliverable:** Basic app with DB connections working âœ…
- âœ… Docker Compose integration in startup scripts
- âœ… Connection retry logic implemented
- âœ… Services automatically managed on startup

### Week 2: Webhook Integration
- [x] GitHub webhook endpoint (`POST /webhooks/github`)
  - âœ… Endpoint created at `/webhooks/github`
  - âœ… Handles POST requests with proper headers
- [x] Signature verification (HMAC SHA-256)
  - âœ… HMAC SHA-256 verification implemented
  - âœ… Constant-time comparison to prevent timing attacks
  - âœ… Optional verification for development
- [x] Parse PR payloads
  - âœ… Extracts PR number, repository name, action
  - âœ… Handles pull_request events (opened, synchronize, reopened)
  - âœ… Validates payload structure
- [x] Store PR metadata in PostgreSQL
  - âœ… Stores new PRs in database
  - âœ… Updates existing PRs on subsequent events
  - âœ… Uses asyncpg for async database operations
  - âœ… Comprehensive error handling

**Progress:** 100% complete âœ…
**Deliverable:** Endpoint receives webhooks and stores data âœ…
- âœ… GitHub App successfully configured and receiving events
- âœ… FastAPI endpoint working and processing webhooks

### Week 3: Job Queue
- [x] Redis Streams producer (enqueue jobs)
  - âœ… Producer implemented with XADD
  - âœ… Job data serialization to JSON
  - âœ… Error handling and logging
- [x] Redis Streams consumer (worker loop)
  - âœ… Consumer loop with XREADGROUP
  - âœ… Message processing and acknowledgment
  - âœ… Pending message handling
  - âœ… Blocking reads with timeout
- [x] Job status tracking
  - âœ… Database status updates at each stage
  - âœ… Timestamp tracking
  - âœ… Status transitions logged
- [x] Worker lifecycle (startup/shutdown)
  - âœ… Signal handlers (SIGINT, SIGTERM)
  - âœ… Graceful shutdown
  - âœ… Startup scripts
- [x] Error handling & retries
  - âœ… Max 3 retries with exponential backoff
  - âœ… Dead letter queue
- [x] Observability & monitoring
  - âœ… Metrics endpoint
  - âœ… Admin dashboard with HTML UI
  - âœ… Structured logging

**Progress:** 100% complete âœ…
**Deliverable:** Jobs flow webhook â†’ queue â†’ worker âœ…
- âœ… Webhook returns <200ms after enqueueing
- âœ… Worker processes jobs asynchronously
- âœ… Full job lifecycle tracking
- âœ… Admin dashboard for monitoring

### Week 4: LLM Integration
- [x] Fetch PR diff from GitHub API
  - âœ… GitHub client with App authentication
  - âœ… Diff fetching and preprocessing
- [x] Multi-provider LLM support (Claude, OpenAI, Zhipu)
  - âœ… Provider abstraction interface
  - âœ… Anthropic (Claude) provider
  - âœ… OpenAI provider
  - âœ… Zhipu provider (for development)
- [x] Parse LLM response to structured comments
  - âœ… Response parser with code snippet support
  - âœ… Severity classification (critical, high, medium, low)
  - âœ… Issue grouping to avoid repetition
- [x] Post review comments to GitHub
  - âœ… GitHub App authentication
  - âœ… Comment formatting with severity badges
  - âœ… Code snippet support in comments
- [x] Error handling (API failures, rate limits)
  - âœ… Comprehensive error types
  - âœ… Retry logic in worker
- [x] Enhanced code review capabilities
  - âœ… Timeout detection for network requests
  - âœ… Database resource leak detection
  - âœ… Code quality improvement suggestions
  - âœ… Security vulnerability detection

**Deliverable:** Full flow working end-to-end âœ…

---

## Month 2: Optimization (Weeks 5-8)

### Week 5: Content-Addressable Caching
- [ ] Cache key generation (SHA-256 hash)
- [ ] Check cache before LLM call
- [ ] Store results with 7-day TTL
- [ ] Track cache hit rate

**Deliverable:** Caching reduces redundant LLM calls

### Week 6: Concurrent Processing
- [ ] Bounded concurrency (asyncio semaphore, max 5)
- [ ] Token bucket rate limiter
- [ ] Exponential backoff for retries
- [ ] Handle GitHub API rate limits

**Deliverable:** System handles 50+ concurrent PRs

### Week 7: Metrics & Observability
- [ ] Structured logging (JSON format)
- [ ] Track: processing time, cache hit rate, API cost, errors
- [ ] Health check endpoint (check dependencies)
- [ ] Optional: Prometheus metrics

**Deliverable:** System is observable

### Week 8: Error Handling
- [ ] Retry logic for transient failures
- [ ] Dead letter queue for permanent failures
- [ ] Graceful shutdown
- [ ] Edge case handling (empty PR, binary files, huge PRs)

**Deliverable:** System handles failures gracefully

---

## Month 3: Production Hardening (Weeks 9-12)

### Week 9: Testing
- [ ] Unit tests (70%+ coverage)
- [ ] Integration tests (webhook â†’ review flow)
- [ ] Mock external APIs (GitHub, OpenAI)
- [ ] Error scenario tests

**Deliverable:** Test suite passing with 70%+ coverage

### Week 10: Load Testing
- [ ] Write locust load test script
- [ ] Test 10, 50, 100 concurrent PRs
- [ ] Measure: throughput, latency (p50/p95/p99), error rate
- [ ] Optimize bottlenecks

**Deliverable:** System proven to handle 50+ PRs, <5s p95 latency

### Week 11: Real Usage
- [x] Deploy to Railway/Render
  - âœ… Railway web service deployed
  - âœ… Railway worker service deployed with health check
  - âœ… GitHub App webhook configured to Railway URL
  - âœ… Database auto-migrations working
  - âœ… Worker health check server integrated
- [ ] Install on your repositories (in progress)
- [ ] Review 50+ PRs across 3 projects
- [ ] Collect data: bugs found, false positives, costs
- [ ] Iterate on prompts based on feedback

**Progress:** 50% complete âœ…
**Deliverable:** System deployed and running on Railway âœ…

### Week 12: Documentation
- [x] README (overview, architecture diagram, setup, examples)
  - âœ… Project overview and architecture
  - âœ… Setup instructions
  - âœ… Tech stack documented
  - âœ… Project structure documented
- [ ] API documentation (OpenAPI/Swagger)
  - âœ… Auto-generated via FastAPI (available at /docs)
  - [ ] Custom documentation improvements needed
- [ ] Architecture Decision Records (ADRs)
- [ ] Demo video (3-5 minutes)
- [x] Clean commit history
  - âœ… Initial commit with proper structure
  - âœ… Documentation commit
- [ ] CI badge, screenshots

**Deliverable:** Professional, demo-ready repository (60% complete)

---

## Database Schema

```sql
-- Pull requests metadata
CREATE TABLE pull_requests (
  id SERIAL PRIMARY KEY,
  pr_number INT NOT NULL,
  repo_full_name VARCHAR(255) NOT NULL,
  status VARCHAR(50) NOT NULL,  -- pending, processing, completed, failed
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);

-- Review comments posted
CREATE TABLE reviews (
  id SERIAL PRIMARY KEY,
  pr_id INT REFERENCES pull_requests(id),
  file_path VARCHAR(500),
  line_number INT,
  comment_text TEXT,
  posted_at TIMESTAMP DEFAULT NOW()
);

-- Track feedback acceptance (optional, for learning)
CREATE TABLE review_feedback (
  id SERIAL PRIMARY KEY,
  review_id INT REFERENCES reviews(id),
  feedback_type VARCHAR(50),  -- accepted, ignored, helpful, unhelpful
  recorded_at TIMESTAMP DEFAULT NOW()
);
```

---

## Key Code Patterns

### Webhook Signature Verification
```python
def verify_signature(payload: bytes, signature: str, secret: str) -> bool:
    expected = hmac.new(secret.encode(), payload, hashlib.sha256).hexdigest()
    return hmac.compare_digest(f"sha256={expected}", signature)
```

### Job Queue (Redis Streams)
```python
# Producer
async def enqueue_job(pr_id: int):
    await redis.xadd("review_jobs", {"pr_id": pr_id})

# Consumer
async def consume_jobs():
    while True:
        messages = await redis.xread({"review_jobs": ">"}, count=1, block=5000)
        for stream, msg_list in messages:
            for msg_id, data in msg_list:
                await process_review(data["pr_id"])
```

### Content-Addressable Cache
```python
def get_cache_key(file_path: str, content: str) -> str:
    hash = hashlib.sha256(content.encode()).hexdigest()[:12]
    return f"review:{file_path}:{hash}"

# Check cache
cache_key = get_cache_key(file_path, content)
cached = await redis.get(cache_key)
if cached:
    return json.loads(cached)  # Cache hit

# Cache miss - call LLM and store
result = await analyze_with_llm(content)
await redis.setex(cache_key, 604800, json.dumps(result))  # 7 day TTL
```

### Bounded Concurrency
```python
class ReviewWorker:
    def __init__(self, max_concurrent: int = 5):
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def process_pr(self, pr_id: int):
        async with self.semaphore:
            await self._do_review(pr_id)
```

### Token Bucket Rate Limiter
```python
class RateLimiter:
    def __init__(self, tokens_per_second: int):
        self.tokens = tokens_per_second
        self.rate = tokens_per_second
        self.last_update = time.time()
    
    async def acquire(self):
        while self.tokens < 1:
            await self._refill()
            await asyncio.sleep(0.1)
        self.tokens -= 1
    
    async def _refill(self):
        now = time.time()
        elapsed = now - self.last_update
        self.tokens = min(self.rate, self.tokens + elapsed * self.rate)
        self.last_update = now
```

---

## Success Criteria

### Functional Completeness
- âœ… Webhook â†’ queue â†’ process â†’ post comment (end-to-end works)
- âœ… Caching reduces redundant API calls
- âœ… Handles 50+ concurrent PRs without failures

### Code Quality
- âœ… 70%+ test coverage
- âœ… Clean, documented code
- âœ… Professional README with architecture diagram

### Real Usage
- âœ… Used on 50+ actual PRs
- âœ… Concrete metrics (cost, latency, bugs found)
- âœ… Examples of bugs caught with screenshots

### Interview Readiness
- âœ… Can explain technical decisions clearly
- âœ… Can demo in <5 minutes
- âœ… Can discuss trade-offs and improvements

---

## Key Metrics to Track

### Performance
- **Throughput:** PRs processed per minute
- **Latency:** p50, p95, p99 processing time
- **Cache Hit Rate:** % of LLM calls avoided

### Cost
- **Total Spent:** $ across all PRs
- **Cost Per PR:** Average $ per review
- **Cache Savings:** $ saved by caching

### Reliability
- **Success Rate:** % of PRs successfully reviewed
- **Error Recovery:** % of failures auto-recovered

### Real Usage
- **PRs Reviewed:** Total count across repositories
- **Bugs Found:** Count with specific examples
- **False Positive Rate:** % of unhelpful comments

---

## Interview Talking Points

**Event-Driven Architecture:**  
"Built event-driven system using GitHub webhooks and Redis Streams. Webhook responses stay under 200ms by queuing long-running analysis asynchronously."

**Concurrency:**  
"Used asyncio with bounded concurrency (semaphore limiting 5 simultaneous PRs) to maximize throughput without overwhelming APIs. Measured p95 latency at 4.1s for 50 concurrent PRs."

**Caching:**  
"Implemented content-addressable caching using SHA-256 hashes. Same file content reuses cached analysis, reducing API costs by 65%."

**Resilience:**  
"Built token bucket rate limiter with exponential backoff. System auto-recovers 87% of transient failures through retry logic."

**Testing:**  
"Comprehensive test suite with pytest-asyncio, mocking external APIs. Load tested with locust proving 50+ concurrent PR capacity."

---

## What We're NOT Building

| Feature | Why Not |
|---------|---------|
| AST parsing | Too complex, LLM handles it |
| Priority queue | FIFO is sufficient |
| Event sourcing | Over-engineered for scope |
| Adaptive rate limiting | Fixed bucket is simpler |
| Multi-language support | Focus on Python/JS |

---

## Dependencies

```toml
# pyproject.toml (Poetry)
[tool.poetry.dependencies]
python = "^3.11"
fastapi = "^0.104.0"
uvicorn = "^0.24.0"
asyncpg = "^0.29.0"  # PostgreSQL async driver
redis = "^5.0.0"
httpx = "^0.25.0"  # Async HTTP client
openai = "^1.3.0"  # or anthropic
pydantic = "^2.5.0"
python-dotenv = "^1.0.0"
structlog = "^23.2.0"

[tool.poetry.dev-dependencies]
pytest = "^7.4.0"
pytest-asyncio = "^0.21.0"
pytest-cov = "^4.1.0"
locust = "^2.17.0"
respx = "^0.20.0"  # HTTP mocking
```

---

## ðŸ“ˆ é¡¹ç›®è¿›åº¦

**æœ€åŽæ›´æ–°:** 2025-11-04

### æ€»ä½“è¿›åº¦

| é˜¶æ®µ | å®Œæˆåº¦ | çŠ¶æ€ |
|------|--------|------|
| Week 1: Foundation | 100% | âœ… å®Œæˆ |
| Week 2: Webhook Integration | 100% | âœ… å®Œæˆ |
| Week 3: Job Queue | 100% | âœ… å®Œæˆ |
| Week 4: LLM Integration | 100% | âœ… å®Œæˆ |
| Week 5-8: Optimization | 0% | ðŸ”„ å¾…å¼€å§‹ |
| Week 9-12: Production Hardening | 0% | ðŸ”„ å¾…å¼€å§‹ |

**æ€»ä½“å®Œæˆåº¦:** ~33% (Week 1-4 å®Œæˆï¼ŒWeek 5-12 å¾…å¼€å§‹)

### è¯¦ç»†å®Œæˆæƒ…å†µ

#### Week 1: Foundation (100% âœ…)
- âœ… Poetry é…ç½®å’Œä¾èµ–ç®¡ç†
- âœ… FastAPI åº”ç”¨éª¨æž¶å’Œé¡¹ç›®ç»“æž„
- âœ… Docker Compose é…ç½®ï¼ˆPostgreSQL 15, Redis 7ï¼‰
- âœ… PostgreSQL schema å’Œè¿ç§»æ–‡ä»¶
- âœ… æ•°æ®åº“å’Œ Redis è¿žæŽ¥æ¨¡å—
- âœ… çŽ¯å¢ƒé…ç½®ï¼ˆPydantic Settingsï¼‰
- âœ… ç»“æž„åŒ–æ—¥å¿—ï¼ˆstructlogï¼‰
- âœ… Git ä»“åº“åˆå§‹åŒ–å’Œ GitHub æŽ¨é€
- âœ… å¯åŠ¨è„šæœ¬å’Œ Docker æœåŠ¡è‡ªåŠ¨ç®¡ç†
- âœ… è¿žæŽ¥é‡è¯•é€»è¾‘

#### Week 2: Webhook Integration (100% âœ…)
- âœ… GitHub webhook ç«¯ç‚¹ (`POST /webhooks/github`)
- âœ… HMAC SHA-256 ç­¾åéªŒè¯
- âœ… PR payload è§£æžå’ŒéªŒè¯
- âœ… PR å…ƒæ•°æ®å­˜å‚¨åˆ° PostgreSQL
- âœ… GitHub App é…ç½®å’Œäº‹ä»¶æŽ¥æ”¶
- âœ… é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

#### Week 3: Job Queue (100% âœ…)
- âœ… Redis Streams producer (XADD)
- âœ… Redis Streams consumer (XREADGROUP)
- âœ… Job çŠ¶æ€è·Ÿè¸ªå’Œæ•°æ®åº“æ›´æ–°
- âœ… Worker ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆä¿¡å·å¤„ç†ã€ä¼˜é›…å…³é—­ï¼‰
- âœ… é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘ï¼ˆæœ€å¤š 3 æ¬¡ï¼ŒæŒ‡æ•°é€€é¿ï¼‰
- âœ… Dead letter queue
- âœ… ç®¡ç†ä»ªè¡¨æ¿ï¼ˆHTML UIï¼‰
- âœ… Metrics ç«¯ç‚¹å’Œå¯è§‚æµ‹æ€§

#### Week 4: LLM Integration (100% âœ…)
- âœ… å¤šæä¾›å•† LLM æŠ½è±¡ï¼ˆClaude, OpenAI, Zhipuï¼‰
- âœ… GitHub App è®¤è¯ï¼ˆJWT + installation tokenï¼‰
- âœ… PR diff èŽ·å–å’Œé¢„å¤„ç†
- âœ… LLM å“åº”è§£æžï¼ˆä¸¥é‡æ€§åˆ†ç±»ã€ä»£ç ç‰‡æ®µã€é—®é¢˜åˆ†ç»„ï¼‰
- âœ… è¯„è®ºå‘å¸ƒåˆ° GitHub PRï¼ˆå¸¦ä¸¥é‡æ€§å¾½ç« å’Œä»£ç ç‰‡æ®µï¼‰
- âœ… å¢žå¼ºçš„ä»£ç å®¡æŸ¥èƒ½åŠ›ï¼š
  - âœ… ç½‘ç»œè¯·æ±‚è¶…æ—¶æ£€æµ‹
  - âœ… æ•°æ®åº“èµ„æºæ³„æ¼æ£€æµ‹
  - âœ… ä»£ç è´¨é‡æ”¹è¿›å»ºè®®
  - âœ… å®‰å…¨æ¼æ´žæ£€æµ‹
- âœ… ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•

### å·²çŸ¥é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### âœ… æ•°æ®åº“è¿žæŽ¥é—®é¢˜ï¼ˆå·²è§£å†³ï¼‰
**é—®é¢˜:** åº”ç”¨å¯åŠ¨æ—¶æ— æ³•è¿žæŽ¥åˆ° PostgreSQL  
**è§£å†³æ–¹æ¡ˆ:**
- âœ… æ·»åŠ è¿žæŽ¥é‡è¯•é€»è¾‘ï¼ˆ5 æ¬¡é‡è¯•ï¼Œæ¯æ¬¡é—´éš” 2 ç§’ï¼‰
- âœ… åˆ›å»ºå¯åŠ¨è„šæœ¬è‡ªåŠ¨ç®¡ç† Docker Compose æœåŠ¡
- âœ… `start.sh` å’Œ `scripts/start-dev.sh` è‡ªåŠ¨å¯åŠ¨å¹¶ç­‰å¾… Docker æœåŠ¡å°±ç»ª

**ä½¿ç”¨æ–¹æ³•:**
```bash
# æŽ¨èæ–¹å¼ï¼šè‡ªåŠ¨å¯åŠ¨ Docker æœåŠ¡
./scripts/start-dev.sh

# æˆ–ä½¿ç”¨ç®€å•å¯åŠ¨è„šæœ¬
./start.sh
```

### å…³é”®æŒ‡æ ‡

#### ä»£ç ç»Ÿè®¡
- **æ€»æ–‡ä»¶æ•°:** 25+
- **ä»£ç è¡Œæ•°:** ~4,000+
- **æµ‹è¯•è¦†ç›–çŽ‡:** 0% (å¾…å¼€å§‹)

#### åŠŸèƒ½å®Œæˆåº¦
- **åŸºç¡€è®¾æ–½:** 100% âœ…
- **Webhook é›†æˆ:** 100% âœ…
- **Job Queue ç³»ç»Ÿ:** 100% âœ…
- **LLM é›†æˆ:** 100% âœ…
- **æ ¸å¿ƒåŠŸèƒ½:** 100% (å®Œæ•´ç«¯åˆ°ç«¯æµç¨‹) âœ…
- **æµ‹è¯•:** 0% (å¾…å¼€å§‹)
- **æ–‡æ¡£:** 85%
- **ç®¡ç†ä»ªè¡¨æ¿:** 100% âœ…

### ç›¸å…³é“¾æŽ¥

- **GitHub ä»“åº“:** https://github.com/kaitlynmi/ai-code-review-agent
- **æœ¬åœ°åº”ç”¨:** http://localhost:8000
- **API æ–‡æ¡£:** http://localhost:8000/docs
- **ç®¡ç†ä»ªè¡¨æ¿:** http://localhost:8000/admin

---

## Project Timeline

- **Month 1:** Core system working end-to-end (Week 1: 100%, Week 2: 100%, Week 3: 100%, Week 4: 100% âœ…)
- **Month 2:** Optimization (caching, concurrency, observability) (0%)
- **Month 3:** Testing, real usage, documentation (0%)

**Total:** 12 weeks to demo-ready portfolio project (33% complete)